{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f969911d-66c2-4e7d-8246-8da98af9b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model, load_model # for creating a Neural Network Autoencoder model\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for adding layers to AE model\n",
    "from tensorflow.keras.utils import plot_model #for plotting  model charts\n",
    "from tensorflow.keras import models,layers,activations,losses,optimizers,metrics\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, TransformerMixin\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d41af81-3f38-452f-8e42-ae312b613b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oad data\n",
    "train_data = pd.read_pickle(\"EDA_train_median.pkl\")\n",
    "test_data = pd.read_pickle(\"EDA_test_median.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c889f4bd-6ce2-4caa-ba6c-2dc1e4fa5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate target variable\n",
    "y = train_data['windmill_generated_power(kW/h)']\n",
    "train_data.drop(['windmill_generated_power(kW/h)'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f306715-2f00-485f-80e2-27b5eee655ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your data\n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Transform the data using the scaler\n",
    "scaled_data = scaler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e8be9f-d692-49d2-8568-66526d93d248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21051, 33), (21051,), (7018, 33), (7018,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, random_state=0)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f3f2a-a055-46f9-8acc-43363ec1a55e",
   "metadata": {},
   "source": [
    "### GENERAL AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a0a64e-e5d1-4d53-9367-66237986e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (InputLayer)    [(None, 33)]              0         \n",
      "                                                                 \n",
      " Encoder-Layer1 (Dense)      (None, 200)               6800      \n",
      "                                                                 \n",
      " Encoder-Layer2 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Bottleneck-Layer (Dense)    (None, 50)                5050      \n",
      "                                                                 \n",
      " Decoder-Layer1 (Dense)      (None, 100)               5100      \n",
      "                                                                 \n",
      " Decoder-Layer2 (Dense)      (None, 200)               20200     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,883\n",
      "Trainable params: 63,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "1316/1316 - 4s - loss: 0.5445 - val_loss: 0.5243 - 4s/epoch - 3ms/step\n",
      "Epoch 2/30\n",
      "1316/1316 - 4s - loss: 0.5188 - val_loss: 0.5179 - 4s/epoch - 3ms/step\n",
      "Epoch 3/30\n",
      "1316/1316 - 3s - loss: 0.5133 - val_loss: 0.5129 - 3s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "1316/1316 - 5s - loss: 0.5098 - val_loss: 0.5100 - 5s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "1316/1316 - 3s - loss: 0.5075 - val_loss: 0.5079 - 3s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "1316/1316 - 3s - loss: 0.5067 - val_loss: 0.5084 - 3s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "1316/1316 - 3s - loss: 0.5061 - val_loss: 0.5072 - 3s/epoch - 2ms/step\n",
      "Epoch 8/30\n",
      "1316/1316 - 3s - loss: 0.5057 - val_loss: 0.5089 - 3s/epoch - 2ms/step\n",
      "Epoch 9/30\n",
      "1316/1316 - 3s - loss: 0.5054 - val_loss: 0.5071 - 3s/epoch - 2ms/step\n",
      "Epoch 10/30\n",
      "1316/1316 - 3s - loss: 0.5051 - val_loss: 0.5071 - 3s/epoch - 3ms/step\n",
      "Epoch 11/30\n",
      "1316/1316 - 4s - loss: 0.5050 - val_loss: 0.5063 - 4s/epoch - 3ms/step\n",
      "Epoch 12/30\n",
      "1316/1316 - 3s - loss: 0.5049 - val_loss: 0.5062 - 3s/epoch - 2ms/step\n",
      "Epoch 13/30\n",
      "1316/1316 - 3s - loss: 0.5048 - val_loss: 0.5061 - 3s/epoch - 2ms/step\n",
      "Epoch 14/30\n",
      "1316/1316 - 3s - loss: 0.5047 - val_loss: 0.5063 - 3s/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "1316/1316 - 3s - loss: 0.5046 - val_loss: 0.5068 - 3s/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "1316/1316 - 3s - loss: 0.5045 - val_loss: 0.5063 - 3s/epoch - 3ms/step\n",
      "Epoch 17/30\n",
      "1316/1316 - 3s - loss: 0.5045 - val_loss: 0.5071 - 3s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "1316/1316 - 3s - loss: 0.5044 - val_loss: 0.5057 - 3s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "1316/1316 - 3s - loss: 0.5043 - val_loss: 0.5056 - 3s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "1316/1316 - 3s - loss: 0.5043 - val_loss: 0.5068 - 3s/epoch - 2ms/step\n",
      "Epoch 21/30\n",
      "1316/1316 - 3s - loss: 0.5042 - val_loss: 0.5065 - 3s/epoch - 3ms/step\n",
      "Epoch 22/30\n",
      "1316/1316 - 3s - loss: 0.5043 - val_loss: 0.5060 - 3s/epoch - 3ms/step\n",
      "Epoch 23/30\n",
      "1316/1316 - 3s - loss: 0.5042 - val_loss: 0.5060 - 3s/epoch - 2ms/step\n",
      "Epoch 24/30\n",
      "1316/1316 - 4s - loss: 0.5041 - val_loss: 0.5061 - 4s/epoch - 3ms/step\n",
      "Epoch 25/30\n",
      "1316/1316 - 4s - loss: 0.5039 - val_loss: 0.5054 - 4s/epoch - 3ms/step\n",
      "Epoch 26/30\n",
      "1316/1316 - 4s - loss: 0.5041 - val_loss: 0.5061 - 4s/epoch - 3ms/step\n",
      "Epoch 27/30\n",
      "1316/1316 - 3s - loss: 0.5041 - val_loss: 0.5059 - 3s/epoch - 3ms/step\n",
      "Epoch 28/30\n",
      "1316/1316 - 4s - loss: 0.5040 - val_loss: 0.5060 - 4s/epoch - 3ms/step\n",
      "Epoch 29/30\n",
      "1316/1316 - 4s - loss: 0.5041 - val_loss: 0.5050 - 4s/epoch - 3ms/step\n",
      "Epoch 30/30\n",
      "1316/1316 - 3s - loss: 0.5038 - val_loss: 0.5055 - 3s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bfb1bd3f10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1] # number of input neurons = the number of features in X_train\n",
    "n_bottleneck= 50\n",
    "\n",
    "#--- Input Layer \n",
    "visible = Input(shape=(n_inputs,), name='Input-Layer') # Specify input shape\n",
    "\n",
    "#--- Encoder Layer\n",
    "e = Dense(units=200, name='Encoder-Layer1',activation=activations.relu, activity_regularizer=regularizers.l1(1e-3))(visible)\n",
    "e = Dense(units=100, name='Encoder-Layer2',activation=activations.relu)(e)\n",
    "\n",
    "#--- Bottleneck\n",
    "bottleneck = Dense(units=n_bottleneck, name='Bottleneck-Layer')(e)\n",
    "\n",
    "#--- Decoder Layer\n",
    "d = Dense(units=100, name='Decoder-Layer1',activation=activations.relu)(bottleneck)\n",
    "d = Dense(units=200, name='Decoder-Layer2',activation=activations.relu)(d)\n",
    "\n",
    "#--- Output layer\n",
    "output = Dense(units=n_inputs, activation='relu', name='Output-Layer')(d)\n",
    "\n",
    "# Define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output, name='Autoencoder-Model')\n",
    "\n",
    "# Compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Train the autoencoder model\n",
    "model.fit(X_train, X_train, epochs=30, batch_size=16, verbose=2, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc5857-6fa7-4fd5-9b2a-bb3246f67728",
   "metadata": {},
   "source": [
    "### Random Forest and General Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95afc901-f2a5-4de3-8e2b-a83199927ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 2s 2ms/step\n",
      "658/658 [==============================] - 1s 1ms/step\n",
      "220/220 [==============================] - 0s 2ms/step\n",
      "MAE: 1.3472008127604307\n",
      "MSE: 3.374159797108745\n",
      "RMSE: 1.836888618590889\n",
      "Train score: 0.5588543793634435\n",
      "Test score: 0.5415254827969815\n"
     ]
    }
   ],
   "source": [
    "X_encoded = model.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode = model.predict(X_train)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode = model.predict(X_test)\n",
    "\n",
    "# Train the SVR model using the encoded features\n",
    "pipe = Pipeline([('SVM',SVR(kernel='rbf'))])\n",
    "\n",
    "# fit the pipeline to our training data\n",
    "pipe.fit(X_train_encode, y_train)\n",
    "\n",
    "#reshape test data\n",
    "#X_test_encode_new = X_test_encode.reshape(-1, 1)\n",
    "#y_test_new = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# prediction \n",
    "pred = pipe.predict(X_test_encode)\n",
    "\n",
    "#metrics \n",
    "print('MAE:', mean_absolute_error(y_test,pred))\n",
    "print('MSE:', mean_squared_error(y_test, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "print('Train score:',pipe.score(X_train_encode,y_train))\n",
    "print('Test score:',pipe.score(X_test_encode,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661fe9d-1fde-457e-9292-d400a8c4fde0",
   "metadata": {},
   "source": [
    "### Decision tree and Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "533e2f20-9929-4b34-8b7f-e924bcdd8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of X_train:  (21051, 33)\n",
      "New shape of X_test:  (7018, 33)\n",
      "New shape of X_train_noisy:  (21051, 33)\n",
      "New shape of X_test_noisy:  (7018, 33)\n",
      "Model: \"Autoencoder-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (InputLayer)    [(None, 33)]              0         \n",
      "                                                                 \n",
      " Encoder-Layer1 (Dense)      (None, 200)               6800      \n",
      "                                                                 \n",
      " Encoder-Layer2 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Bottleneck-Layer (Dense)    (None, 50)                5050      \n",
      "                                                                 \n",
      " Decoder-Layer1 (Dense)      (None, 100)               5100      \n",
      "                                                                 \n",
      " Decoder-Layer2 (Dense)      (None, 200)               20200     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,883\n",
      "Trainable params: 63,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "658/658 [==============================] - 3s 4ms/step - loss: 0.1018 - val_loss: 0.0845\n",
      "Epoch 2/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0820 - val_loss: 0.0803\n",
      "Epoch 3/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0794 - val_loss: 0.0789\n",
      "Epoch 4/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0778 - val_loss: 0.0769\n",
      "Epoch 5/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0770 - val_loss: 0.0764\n",
      "Epoch 6/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0764 - val_loss: 0.0764\n",
      "Epoch 7/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0760 - val_loss: 0.0758\n",
      "Epoch 8/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0757 - val_loss: 0.0754\n",
      "Epoch 9/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0756 - val_loss: 0.0755\n",
      "Epoch 10/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0754 - val_loss: 0.0755\n",
      "Epoch 11/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0753 - val_loss: 0.0756\n",
      "Epoch 12/30\n",
      "658/658 [==============================] - 2s 4ms/step - loss: 0.0752 - val_loss: 0.0747\n",
      "Epoch 13/30\n",
      "658/658 [==============================] - 2s 4ms/step - loss: 0.0751 - val_loss: 0.0756\n",
      "Epoch 14/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0750 - val_loss: 0.0748\n",
      "Epoch 15/30\n",
      "658/658 [==============================] - 2s 4ms/step - loss: 0.0750 - val_loss: 0.0751\n",
      "Epoch 16/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0749 - val_loss: 0.0748\n",
      "Epoch 17/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0748 - val_loss: 0.0748\n",
      "Epoch 18/30\n",
      "658/658 [==============================] - 2s 4ms/step - loss: 0.0749 - val_loss: 0.0749\n",
      "Epoch 19/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0748 - val_loss: 0.0752\n",
      "Epoch 20/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0748 - val_loss: 0.0747\n",
      "Epoch 21/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0747 - val_loss: 0.0752\n",
      "Epoch 22/30\n",
      "658/658 [==============================] - 3s 4ms/step - loss: 0.0746 - val_loss: 0.0749\n",
      "Epoch 23/30\n",
      "658/658 [==============================] - 2s 4ms/step - loss: 0.0746 - val_loss: 0.0743\n",
      "Epoch 24/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0747 - val_loss: 0.0747\n",
      "Epoch 25/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0746 - val_loss: 0.0741\n",
      "Epoch 26/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0745 - val_loss: 0.0744\n",
      "Epoch 27/30\n",
      "658/658 [==============================] - 2s 4ms/step - loss: 0.0746 - val_loss: 0.0747\n",
      "Epoch 28/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0745 - val_loss: 0.0750\n",
      "Epoch 29/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0744 - val_loss: 0.0746\n",
      "Epoch 30/30\n",
      "658/658 [==============================] - 2s 3ms/step - loss: 0.0744 - val_loss: 0.0742\n",
      "878/878 [==============================] - 2s 2ms/step\n",
      "658/658 [==============================] - 1s 1ms/step\n",
      "220/220 [==============================] - 0s 1ms/step\n",
      "RMSE: 1.840849347638716\n",
      "Mean Absolute Error: 1.3337614099421702\n",
      "Mean Squared Error: 3.3887263207018865\n",
      "Train score: 0.5538498288791152\n",
      "Test score: 0.5395462108379552\n"
     ]
    }
   ],
   "source": [
    "# Specify how much noise to add\n",
    "level_of_noise = 0.00001\n",
    "\n",
    "# Add random noise based on sampling from Gaussian distribution\n",
    "X_train_noisy = X_train + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "X_test_noisy = X_test + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "\n",
    "# Enforce min-max boundaries so it does not go beyond [0,1] range\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)\n",
    "# Print shapes\n",
    "print(\"New shape of X_train: \", X_train.shape)\n",
    "print(\"New shape of X_test: \", X_test.shape)\n",
    "\n",
    "print(\"New shape of X_train_noisy: \", X_train_noisy.shape)\n",
    "print(\"New shape of X_test_noisy: \", X_test_noisy.shape)\n",
    "\n",
    "\n",
    "#--- Define Shapes\n",
    "n_inputs = X_train_noisy.shape[1] # number of input neurons = the number of features in X_train\n",
    "n_bottleneck= 50\n",
    "\n",
    "#--- Input Layer \n",
    "visible = Input(shape=(n_inputs,), name='Input-Layer') # Specify input shape\n",
    "\n",
    "#--- Encoder Layer\n",
    "e = Dense(units=200, name='Encoder-Layer1',activation=activations.relu, activity_regularizer=regularizers.l1(1e-3))(visible)\n",
    "e = Dense(units=100, name='Encoder-Layer2',activation=activations.relu)(e)\n",
    "\n",
    "#--- Bottleneck\n",
    "bottleneck = Dense(units=n_bottleneck, name='Bottleneck-Layer')(e)\n",
    "\n",
    "#--- Decoder Layer\n",
    "d = Dense(units=100, name='Decoder-Layer1',activation=activations.relu)(bottleneck)\n",
    "d = Dense(units=200, name='Decoder-Layer2',activation=activations.relu)(d)\n",
    "\n",
    "#--- Output layer\n",
    "output = Dense(units=n_inputs, activation='relu', name='Output-Layer')(d)\n",
    "\n",
    "# Define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output, name='Autoencoder-Model')\n",
    "\n",
    "# Compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train_noisy, X_train_noisy, epochs=30, batch_size=32, verbose=1, validation_data=(X_test_noisy, X_test_noisy))\n",
    "\n",
    "X_encoded_denoise = model.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode_denoise = model.predict(X_train_noisy)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode_denoise = model.predict(X_test_noisy)\n",
    "\n",
    "# Train the SVR model using the encoded features\n",
    "pipe_denoise = Pipeline([('SVM',SVR(kernel='rbf'))])\n",
    "\n",
    "# fit the pipeline to our training data\n",
    "pipe_denoise.fit(X_train_encode_denoise, y_train)\n",
    "\n",
    "#test prediction\n",
    "prediction_denoise = pipe_denoise.predict(X_test_encode_denoise)\n",
    "\n",
    "# Calculate the MAE and MSE\n",
    "mae = mean_absolute_error(y_test, prediction_denoise)\n",
    "mse = mean_squared_error(y_test, prediction_denoise)\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, prediction_denoise)))\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print('Train score:',pipe_denoise.score(X_train_encode_denoise,y_train))\n",
    "print('Test score:',pipe_denoise.score(X_test_encode_denoise,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53afb6-c184-4f23-a394-ae688a641a2a",
   "metadata": {},
   "source": [
    "### Random Forest and Variational Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb868450-346e-4451-8fa4-6341519d0087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 33)]         0           []                               \n",
      "                                                                                                  \n",
      " encoding1 (Dense)              (None, 200)          6800        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " encoding (Dense)               (None, 100)          20100       ['encoding1[0][0]']              \n",
      "                                                                                                  \n",
      " mean (Dense)                   (None, 50)           5050        ['encoding[0][0]']               \n",
      "                                                                                                  \n",
      " log-variance (Dense)           (None, 50)           5050        ['encoding[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 50)           0           ['mean[0][0]',                   \n",
      "                                                                  'log-variance[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37,000\n",
      "Trainable params: 37,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 50)]              0         \n",
      "                                                                 \n",
      " decoder_h2 (Dense)          (None, 100)               5100      \n",
      "                                                                 \n",
      " decoder_h (Dense)           (None, 200)               20200     \n",
      "                                                                 \n",
      " flat_decoded (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,933\n",
      "Trainable params: 31,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 33)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        [(None, 50),              37000     \n",
      "                              (None, 50),                        \n",
      "                              (None, 50)]                        \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 33)                31933     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,933\n",
      "Trainable params: 68,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21051 samples, validate on 7018 samples\n",
      "Epoch 1/30\n",
      "20864/21051 [============================>.] - ETA: 0s - loss: 12.5286 - mae: 0.3493"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21051/21051 [==============================] - 1s 61us/sample - loss: 12.5012 - mae: 0.3486 - val_loss: 9.8045 - val_mae: 0.2765\n",
      "Epoch 2/30\n",
      "21051/21051 [==============================] - 1s 65us/sample - loss: 9.2690 - mae: 0.2642 - val_loss: 8.9026 - val_mae: 0.2559\n",
      "Epoch 3/30\n",
      "21051/21051 [==============================] - 1s 59us/sample - loss: 8.6951 - mae: 0.2510 - val_loss: 8.4913 - val_mae: 0.2456\n",
      "Epoch 4/30\n",
      "21051/21051 [==============================] - 1s 55us/sample - loss: 8.3791 - mae: 0.2425 - val_loss: 8.2852 - val_mae: 0.2396\n",
      "Epoch 5/30\n",
      "21051/21051 [==============================] - 1s 57us/sample - loss: 8.0844 - mae: 0.2333 - val_loss: 7.9317 - val_mae: 0.2286\n",
      "Epoch 6/30\n",
      "21051/21051 [==============================] - 1s 56us/sample - loss: 7.8434 - mae: 0.2262 - val_loss: 7.7070 - val_mae: 0.2225\n",
      "Epoch 7/30\n",
      "21051/21051 [==============================] - 1s 63us/sample - loss: 7.6629 - mae: 0.2218 - val_loss: 7.5570 - val_mae: 0.2193\n",
      "Epoch 8/30\n",
      "21051/21051 [==============================] - 1s 70us/sample - loss: 7.5540 - mae: 0.2197 - val_loss: 7.5303 - val_mae: 0.2195\n",
      "Epoch 9/30\n",
      "21051/21051 [==============================] - 1s 67us/sample - loss: 7.4513 - mae: 0.2176 - val_loss: 7.4576 - val_mae: 0.2181\n",
      "Epoch 10/30\n",
      "21051/21051 [==============================] - 1s 63us/sample - loss: 7.3873 - mae: 0.2164 - val_loss: 7.2905 - val_mae: 0.2137\n",
      "Epoch 11/30\n",
      "21051/21051 [==============================] - 1s 66us/sample - loss: 7.3469 - mae: 0.2158 - val_loss: 7.2972 - val_mae: 0.2145\n",
      "Epoch 12/30\n",
      "21051/21051 [==============================] - 1s 60us/sample - loss: 7.2994 - mae: 0.2148 - val_loss: 7.2977 - val_mae: 0.2150\n",
      "Epoch 13/30\n",
      "21051/21051 [==============================] - 1s 55us/sample - loss: 7.2667 - mae: 0.2142 - val_loss: 7.2566 - val_mae: 0.2141\n",
      "Epoch 14/30\n",
      "21051/21051 [==============================] - 1s 57us/sample - loss: 7.2321 - mae: 0.2135 - val_loss: 7.2002 - val_mae: 0.2127\n",
      "Epoch 15/30\n",
      "21051/21051 [==============================] - 1s 61us/sample - loss: 7.2248 - mae: 0.2136 - val_loss: 7.2682 - val_mae: 0.2151\n",
      "Epoch 16/30\n",
      "21051/21051 [==============================] - 1s 58us/sample - loss: 7.2026 - mae: 0.2132 - val_loss: 7.2075 - val_mae: 0.2135\n",
      "Epoch 17/30\n",
      "21051/21051 [==============================] - 1s 57us/sample - loss: 7.1771 - mae: 0.2127 - val_loss: 7.1111 - val_mae: 0.2108\n",
      "Epoch 18/30\n",
      "21051/21051 [==============================] - 1s 48us/sample - loss: 7.1665 - mae: 0.2126 - val_loss: 7.1759 - val_mae: 0.2129\n",
      "Epoch 19/30\n",
      "21051/21051 [==============================] - 1s 38us/sample - loss: 7.1590 - mae: 0.2125 - val_loss: 7.1157 - val_mae: 0.2113\n",
      "Epoch 20/30\n",
      "21051/21051 [==============================] - 1s 41us/sample - loss: 7.1270 - mae: 0.2117 - val_loss: 7.0721 - val_mae: 0.2102\n",
      "Epoch 21/30\n",
      "21051/21051 [==============================] - 1s 49us/sample - loss: 7.1118 - mae: 0.2114 - val_loss: 7.1224 - val_mae: 0.2118\n",
      "Epoch 22/30\n",
      "21051/21051 [==============================] - 1s 57us/sample - loss: 7.1106 - mae: 0.2115 - val_loss: 7.0510 - val_mae: 0.2098\n",
      "Epoch 23/30\n",
      "21051/21051 [==============================] - 1s 57us/sample - loss: 7.0995 - mae: 0.2113 - val_loss: 7.1101 - val_mae: 0.2117\n",
      "Epoch 24/30\n",
      "21051/21051 [==============================] - 1s 62us/sample - loss: 7.1038 - mae: 0.2116 - val_loss: 7.0687 - val_mae: 0.2106\n",
      "Epoch 25/30\n",
      "21051/21051 [==============================] - 1s 62us/sample - loss: 7.0847 - mae: 0.2111 - val_loss: 7.0636 - val_mae: 0.2105\n",
      "Epoch 26/30\n",
      "21051/21051 [==============================] - 1s 66us/sample - loss: 7.0621 - mae: 0.2105 - val_loss: 7.0871 - val_mae: 0.2113\n",
      "Epoch 27/30\n",
      "21051/21051 [==============================] - 1s 62us/sample - loss: 7.0776 - mae: 0.2110 - val_loss: 7.1049 - val_mae: 0.2119\n",
      "Epoch 28/30\n",
      "21051/21051 [==============================] - 1s 65us/sample - loss: 7.0695 - mae: 0.2109 - val_loss: 7.0895 - val_mae: 0.2115\n",
      "Epoch 29/30\n",
      "21051/21051 [==============================] - 1s 65us/sample - loss: 7.0694 - mae: 0.2109 - val_loss: 7.0336 - val_mae: 0.2099\n",
      "Epoch 30/30\n",
      "21051/21051 [==============================] - 1s 64us/sample - loss: 7.0562 - mae: 0.2106 - val_loss: 7.0750 - val_mae: 0.2112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.2457134175458782\n",
      "MSE: 2.907982793614754\n",
      "RMSE: 1.7052808547611018\n",
      "Train score: 0.6660852544833167\n",
      "Test score: 0.6048687414035239\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "original_dim = (X_train.shape[1])\n",
    "latent_dim = 50\n",
    "intermediate_dim1 = 200\n",
    "intermediate_dim2 = 100\n",
    "epochs = 30\n",
    "epsilon_std = 0.000001\n",
    "\n",
    "\n",
    "# sampling from mean and sd in VAE\n",
    "def sampling(args: tuple):\n",
    "    # we grab the variables from the tuple\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "#########################\n",
    "# input to our encoder\n",
    "#########################\n",
    "x = Input(shape=(original_dim,), name=\"input\")\n",
    "\n",
    "# intermediate layer\n",
    "#h1 = Dense(intermediate_dim1, activation='tanh', name=\"encoding1\")(x)\n",
    "h1 = Dense(intermediate_dim1, activation=\"tanh\", name=\"encoding1\", activity_regularizer=regularizers.l1(10e-5))(x)\n",
    "h = Dense(intermediate_dim2, activation='tanh', name=\"encoding\")(h1)\n",
    "\n",
    "\n",
    "# defining the mean of the latent space\n",
    "z_mean = Dense(latent_dim, name=\"mean\")(h)\n",
    "\n",
    "# defining the log variance of the latent space\n",
    "z_log_var = Dense(latent_dim, name=\"log-variance\")(h)\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# defining the encoder as a keras model\n",
    "encoder = Model(x, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# print out summary of what we just did\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "#########################\n",
    "# Input to the decoder\n",
    "#########################\n",
    "\n",
    "input_decoder = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
    "\n",
    "# taking the latent space to intermediate dimension\n",
    "decoder_h1 = Dense(intermediate_dim2, activation='relu', name=\"decoder_h2\")(input_decoder)\n",
    "decoder_h = Dense(intermediate_dim1, activation='relu', name=\"decoder_h\")(decoder_h1)\n",
    "\n",
    "# getting the mean from the original dimension\n",
    "x_decoded = Dense(original_dim, activation='tanh', name=\"flat_decoded\")(decoder_h)\n",
    "\n",
    "# defining the decoder as a keras model\n",
    "decoder = Model(input_decoder, x_decoded, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "##########################\n",
    "# Variational Autoencoder\n",
    "##########################\n",
    "\n",
    "# grab the output. Recall, that we need to grab the 3rd element our sampling z\n",
    "output_combined = decoder(encoder(x)[2])\n",
    "\n",
    "# link the input and the overall output\n",
    "vae = Model(x, output_combined)\n",
    "\n",
    "# print out what the overall model looks like\n",
    "vae.summary()\n",
    "\n",
    "# Defina VAE Loss Function\n",
    "def vae_loss(x: tf.Tensor, x_decoded_mean: tf.Tensor,z_log_var=z_log_var, z_mean=z_mean, original_dim=original_dim):\n",
    "    xent_loss = original_dim * metrics.mae(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss, metrics=['mae'],experimental_run_tf_function=False)\n",
    "\n",
    "history = vae.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1).history\n",
    "\n",
    "\n",
    "X_encoded = vae.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode_vae = vae.predict(X_train)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode_vae = vae.predict(X_test)\n",
    "\n",
    "#Train the SVR\n",
    "pipe_vae = Pipeline([('SVM',SVR(kernel='rbf'))])\n",
    "\n",
    "# fit the pipeline to our training data\n",
    "pipe_vae.fit(X_train_encode_vae, y_train)\n",
    "\n",
    "\n",
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2))\n",
    "    #return MAE\n",
    "    return np.mean(abs(v1 - v2))\n",
    "\n",
    "X_train_pred = pipe_vae.predict(X_train_encode_vae)\n",
    "mae_vector_train = get_error_term(X_train_pred, y_train, _rmse=False)\n",
    "                   \n",
    "X_pred = pipe_vae.predict(X_test_encode_vae)\n",
    "mae_vector_test = get_error_term(X_pred, y_test, _rmse=False)\n",
    "                   \n",
    "\n",
    "#metrics \n",
    "print('MAE:', mean_absolute_error(y_test,X_pred))\n",
    "print('MSE:', mean_squared_error(y_test, X_pred))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, X_pred)))\n",
    "print('Train score:',pipe_vae.score(X_train_encode_vae,y_train))\n",
    "print('Test score:',pipe_vae.score(X_test_encode_vae,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6518f65-7310-4fa7-90ee-bc7075db9d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
