{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f969911d-66c2-4e7d-8246-8da98af9b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model, load_model # for creating a Neural Network Autoencoder model\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for adding layers to AE model\n",
    "from tensorflow.keras.utils import plot_model #for plotting  model charts\n",
    "from tensorflow.keras import models,layers,activations,losses,optimizers,metrics\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, TransformerMixin\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d41af81-3f38-452f-8e42-ae312b613b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oad data\n",
    "train_data = pd.read_pickle(\"EDA_train_mean.pkl\")\n",
    "test_data = pd.read_pickle(\"EDA_test_median.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c889f4bd-6ce2-4caa-ba6c-2dc1e4fa5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate target variable\n",
    "y = train_data['windmill_generated_power(kW/h)']\n",
    "train_data.drop(['windmill_generated_power(kW/h)'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f306715-2f00-485f-80e2-27b5eee655ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your data\n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Transform the data using the scaler\n",
    "scaled_data = scaler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e8be9f-d692-49d2-8568-66526d93d248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21055, 33), (21055,), (7019, 33), (7019,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, random_state=0)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f3f2a-a055-46f9-8acc-43363ec1a55e",
   "metadata": {},
   "source": [
    "### GENERAL AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04a0a64e-e5d1-4d53-9367-66237986e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (InputLayer)    [(None, 33)]              0         \n",
      "                                                                 \n",
      " Encoder-Layer1 (Dense)      (None, 200)               6800      \n",
      "                                                                 \n",
      " Encoder-Layer2 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Bottleneck-Layer (Dense)    (None, 50)                5050      \n",
      "                                                                 \n",
      " Decoder-Layer1 (Dense)      (None, 100)               5100      \n",
      "                                                                 \n",
      " Decoder-Layer2 (Dense)      (None, 200)               20200     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,883\n",
      "Trainable params: 63,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "1316/1316 - 4s - loss: 0.5264 - val_loss: 0.5023 - 4s/epoch - 3ms/step\n",
      "Epoch 2/30\n",
      "1316/1316 - 4s - loss: 0.4959 - val_loss: 0.4928 - 4s/epoch - 3ms/step\n",
      "Epoch 3/30\n",
      "1316/1316 - 3s - loss: 0.4888 - val_loss: 0.4868 - 3s/epoch - 2ms/step\n",
      "Epoch 4/30\n",
      "1316/1316 - 4s - loss: 0.4845 - val_loss: 0.4841 - 4s/epoch - 3ms/step\n",
      "Epoch 5/30\n",
      "1316/1316 - 3s - loss: 0.4822 - val_loss: 0.4829 - 3s/epoch - 2ms/step\n",
      "Epoch 6/30\n",
      "1316/1316 - 3s - loss: 0.4809 - val_loss: 0.4816 - 3s/epoch - 3ms/step\n",
      "Epoch 7/30\n",
      "1316/1316 - 4s - loss: 0.4801 - val_loss: 0.4807 - 4s/epoch - 3ms/step\n",
      "Epoch 8/30\n",
      "1316/1316 - 3s - loss: 0.4796 - val_loss: 0.4807 - 3s/epoch - 3ms/step\n",
      "Epoch 9/30\n",
      "1316/1316 - 3s - loss: 0.4794 - val_loss: 0.4811 - 3s/epoch - 3ms/step\n",
      "Epoch 10/30\n",
      "1316/1316 - 3s - loss: 0.4793 - val_loss: 0.4799 - 3s/epoch - 2ms/step\n",
      "Epoch 11/30\n",
      "1316/1316 - 3s - loss: 0.4788 - val_loss: 0.4804 - 3s/epoch - 2ms/step\n",
      "Epoch 12/30\n",
      "1316/1316 - 3s - loss: 0.4788 - val_loss: 0.4796 - 3s/epoch - 2ms/step\n",
      "Epoch 13/30\n",
      "1316/1316 - 3s - loss: 0.4787 - val_loss: 0.4804 - 3s/epoch - 2ms/step\n",
      "Epoch 14/30\n",
      "1316/1316 - 3s - loss: 0.4785 - val_loss: 0.4793 - 3s/epoch - 2ms/step\n",
      "Epoch 15/30\n",
      "1316/1316 - 3s - loss: 0.4784 - val_loss: 0.4789 - 3s/epoch - 2ms/step\n",
      "Epoch 16/30\n",
      "1316/1316 - 3s - loss: 0.4784 - val_loss: 0.4790 - 3s/epoch - 2ms/step\n",
      "Epoch 17/30\n",
      "1316/1316 - 3s - loss: 0.4782 - val_loss: 0.4796 - 3s/epoch - 2ms/step\n",
      "Epoch 18/30\n",
      "1316/1316 - 3s - loss: 0.4783 - val_loss: 0.4799 - 3s/epoch - 2ms/step\n",
      "Epoch 19/30\n",
      "1316/1316 - 3s - loss: 0.4781 - val_loss: 0.4796 - 3s/epoch - 2ms/step\n",
      "Epoch 20/30\n",
      "1316/1316 - 3s - loss: 0.4781 - val_loss: 0.4792 - 3s/epoch - 3ms/step\n",
      "Epoch 21/30\n",
      "1316/1316 - 3s - loss: 0.4780 - val_loss: 0.4795 - 3s/epoch - 2ms/step\n",
      "Epoch 22/30\n",
      "1316/1316 - 3s - loss: 0.4778 - val_loss: 0.4793 - 3s/epoch - 2ms/step\n",
      "Epoch 23/30\n",
      "1316/1316 - 4s - loss: 0.4780 - val_loss: 0.4792 - 4s/epoch - 3ms/step\n",
      "Epoch 24/30\n",
      "1316/1316 - 3s - loss: 0.4778 - val_loss: 0.4795 - 3s/epoch - 2ms/step\n",
      "Epoch 25/30\n",
      "1316/1316 - 4s - loss: 0.4777 - val_loss: 0.4789 - 4s/epoch - 3ms/step\n",
      "Epoch 26/30\n",
      "1316/1316 - 4s - loss: 0.4777 - val_loss: 0.4798 - 4s/epoch - 3ms/step\n",
      "Epoch 27/30\n",
      "1316/1316 - 4s - loss: 0.4776 - val_loss: 0.4790 - 4s/epoch - 3ms/step\n",
      "Epoch 28/30\n",
      "1316/1316 - 4s - loss: 0.4777 - val_loss: 0.4789 - 4s/epoch - 3ms/step\n",
      "Epoch 29/30\n",
      "1316/1316 - 3s - loss: 0.4776 - val_loss: 0.4787 - 3s/epoch - 3ms/step\n",
      "Epoch 30/30\n",
      "1316/1316 - 3s - loss: 0.4773 - val_loss: 0.4782 - 3s/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x27967b1db20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1] # number of input neurons = the number of features in X_train\n",
    "n_bottleneck= 50\n",
    "\n",
    "#--- Input Layer \n",
    "visible = Input(shape=(n_inputs,), name='Input-Layer') # Specify input shape\n",
    "\n",
    "#--- Encoder Layer\n",
    "e = Dense(units=200, name='Encoder-Layer1',activation=activations.relu, activity_regularizer=regularizers.l1(1e-3))(visible)\n",
    "e = Dense(units=100, name='Encoder-Layer2',activation=activations.relu)(e)\n",
    "\n",
    "#--- Bottleneck\n",
    "bottleneck = Dense(units=n_bottleneck, name='Bottleneck-Layer')(e)\n",
    "\n",
    "#--- Decoder Layer\n",
    "d = Dense(units=100, name='Decoder-Layer1',activation=activations.relu)(bottleneck)\n",
    "d = Dense(units=200, name='Decoder-Layer2',activation=activations.relu)(d)\n",
    "\n",
    "#--- Output layer\n",
    "output = Dense(units=n_inputs, activation='relu', name='Output-Layer')(d)\n",
    "\n",
    "# Define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output, name='Autoencoder-Model')\n",
    "\n",
    "# Compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Train the autoencoder model\n",
    "model.fit(X_train, X_train, epochs=30, batch_size=16, verbose=2, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc5857-6fa7-4fd5-9b2a-bb3246f67728",
   "metadata": {},
   "source": [
    "### Random Forest and General Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95afc901-f2a5-4de3-8e2b-a83199927ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "878/878 [==============================] - 1s 1ms/step\n",
      "658/658 [==============================] - 1s 1ms/step\n",
      "220/220 [==============================] - 0s 1ms/step\n",
      "MAE: 1.0244684988319697\n",
      "MSE: 2.0675203153210084\n",
      "RMSE: 1.4378874487667692\n",
      "Train score: 0.9595379966699928\n",
      "Test score: 0.7236431215879027\n"
     ]
    }
   ],
   "source": [
    "X_encoded = model.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode = model.predict(X_train)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode = model.predict(X_test)\n",
    "\n",
    "# Train the random forest model using the encoded features\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train_encode, y_train)\n",
    "\n",
    "#reshape test data\n",
    "#X_test_encode_new = X_test_encode.reshape(-1, 1)\n",
    "#y_test_new = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# prediction \n",
    "pred = rfr.predict(X_test_encode)\n",
    "\n",
    "#metrics \n",
    "print('MAE:', mean_absolute_error(y_test,pred))\n",
    "print('MSE:', mean_squared_error(y_test, pred))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred)))\n",
    "print('Train score:',rfr.score(X_train_encode,y_train))\n",
    "print('Test score:',rfr.score(X_test_encode,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661fe9d-1fde-457e-9292-d400a8c4fde0",
   "metadata": {},
   "source": [
    "### Decision tree and Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "533e2f20-9929-4b34-8b7f-e924bcdd8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of X_train:  (21055, 33)\n",
      "New shape of X_test:  (7019, 33)\n",
      "New shape of X_train_noisy:  (21055, 33)\n",
      "New shape of X_test_noisy:  (7019, 33)\n",
      "Model: \"Autoencoder-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (InputLayer)    [(None, 33)]              0         \n",
      "                                                                 \n",
      " Encoder-Layer1 (Dense)      (None, 200)               6800      \n",
      "                                                                 \n",
      " Encoder-Layer2 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Bottleneck-Layer (Dense)    (None, 50)                5050      \n",
      "                                                                 \n",
      " Decoder-Layer1 (Dense)      (None, 100)               5100      \n",
      "                                                                 \n",
      " Decoder-Layer2 (Dense)      (None, 200)               20200     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,883\n",
      "Trainable params: 63,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21055 samples, validate on 7019 samples\n",
      "Epoch 1/30\n",
      "21055/21055 [==============================] - ETA: 0s - loss: 0.1008"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21055/21055 [==============================] - 2s 90us/sample - loss: 0.1008 - val_loss: 0.0838\n",
      "Epoch 2/30\n",
      "21055/21055 [==============================] - 1s 68us/sample - loss: 0.0816 - val_loss: 0.0801\n",
      "Epoch 3/30\n",
      "21055/21055 [==============================] - 2s 74us/sample - loss: 0.0770 - val_loss: 0.0729\n",
      "Epoch 4/30\n",
      "21055/21055 [==============================] - 2s 95us/sample - loss: 0.0689 - val_loss: 0.0683\n",
      "Epoch 5/30\n",
      "21055/21055 [==============================] - 2s 83us/sample - loss: 0.0673 - val_loss: 0.0676\n",
      "Epoch 6/30\n",
      "21055/21055 [==============================] - 2s 108us/sample - loss: 0.0666 - val_loss: 0.0671\n",
      "Epoch 7/30\n",
      "21055/21055 [==============================] - 2s 97us/sample - loss: 0.0661 - val_loss: 0.0670\n",
      "Epoch 8/30\n",
      "21055/21055 [==============================] - 2s 88us/sample - loss: 0.0657 - val_loss: 0.0658\n",
      "Epoch 9/30\n",
      "21055/21055 [==============================] - 1s 67us/sample - loss: 0.0655 - val_loss: 0.0658\n",
      "Epoch 10/30\n",
      "21055/21055 [==============================] - 1s 70us/sample - loss: 0.0654 - val_loss: 0.0658\n",
      "Epoch 11/30\n",
      "21055/21055 [==============================] - 2s 74us/sample - loss: 0.0652 - val_loss: 0.0663\n",
      "Epoch 12/30\n",
      "21055/21055 [==============================] - 2s 100us/sample - loss: 0.0650 - val_loss: 0.0652\n",
      "Epoch 13/30\n",
      "21055/21055 [==============================] - 2s 93us/sample - loss: 0.0650 - val_loss: 0.0658\n",
      "Epoch 14/30\n",
      "21055/21055 [==============================] - 2s 79us/sample - loss: 0.0650 - val_loss: 0.0656\n",
      "Epoch 15/30\n",
      "21055/21055 [==============================] - 2s 72us/sample - loss: 0.0648 - val_loss: 0.0649\n",
      "Epoch 16/30\n",
      "21055/21055 [==============================] - 2s 91us/sample - loss: 0.0648 - val_loss: 0.0653\n",
      "Epoch 17/30\n",
      "21055/21055 [==============================] - 2s 86us/sample - loss: 0.0648 - val_loss: 0.0652\n",
      "Epoch 18/30\n",
      "21055/21055 [==============================] - 2s 72us/sample - loss: 0.0647 - val_loss: 0.0654\n",
      "Epoch 19/30\n",
      "21055/21055 [==============================] - 1s 65us/sample - loss: 0.0646 - val_loss: 0.0652\n",
      "Epoch 20/30\n",
      "21055/21055 [==============================] - 1s 64us/sample - loss: 0.0646 - val_loss: 0.0654\n",
      "Epoch 21/30\n",
      "21055/21055 [==============================] - 2s 72us/sample - loss: 0.0645 - val_loss: 0.0650\n",
      "Epoch 22/30\n",
      "21055/21055 [==============================] - 2s 102us/sample - loss: 0.0644 - val_loss: 0.0648\n",
      "Epoch 23/30\n",
      "21055/21055 [==============================] - 2s 92us/sample - loss: 0.0644 - val_loss: 0.0645\n",
      "Epoch 24/30\n",
      "21055/21055 [==============================] - 2s 97us/sample - loss: 0.0643 - val_loss: 0.0648\n",
      "Epoch 25/30\n",
      "21055/21055 [==============================] - 2s 76us/sample - loss: 0.0643 - val_loss: 0.0649\n",
      "Epoch 26/30\n",
      "21055/21055 [==============================] - 2s 97us/sample - loss: 0.0642 - val_loss: 0.0647\n",
      "Epoch 27/30\n",
      "21055/21055 [==============================] - 2s 73us/sample - loss: 0.0641 - val_loss: 0.0644\n",
      "Epoch 28/30\n",
      "21055/21055 [==============================] - 1s 67us/sample - loss: 0.0641 - val_loss: 0.0645\n",
      "Epoch 29/30\n",
      "21055/21055 [==============================] - 2s 71us/sample - loss: 0.0642 - val_loss: 0.0646\n",
      "Epoch 30/30\n",
      "21055/21055 [==============================] - 1s 65us/sample - loss: 0.0641 - val_loss: 0.0645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5511600751607546\n",
      "Mean Absolute Error: 1.1300489615264564\n",
      "Mean Squared Error: 2.406097578772718\n",
      "Train score: 0.953237496978968\n",
      "Test score: 0.6783869009184109\n"
     ]
    }
   ],
   "source": [
    "# Specify how much noise to add\n",
    "level_of_noise = 0.01\n",
    "\n",
    "# Add random noise based on sampling from Gaussian distribution\n",
    "X_train_noisy = X_train + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "X_test_noisy = X_test + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "\n",
    "# Enforce min-max boundaries so it does not go beyond [0,1] range\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)\n",
    "# Print shapes\n",
    "print(\"New shape of X_train: \", X_train.shape)\n",
    "print(\"New shape of X_test: \", X_test.shape)\n",
    "\n",
    "print(\"New shape of X_train_noisy: \", X_train_noisy.shape)\n",
    "print(\"New shape of X_test_noisy: \", X_test_noisy.shape)\n",
    "\n",
    "\n",
    "#--- Define Shapes\n",
    "n_inputs = X_train_noisy.shape[1] # number of input neurons = the number of features in X_train\n",
    "n_bottleneck= 50\n",
    "\n",
    "#--- Input Layer \n",
    "visible = Input(shape=(n_inputs,), name='Input-Layer') # Specify input shape\n",
    "\n",
    "#--- Encoder Layer\n",
    "e = Dense(units=200, name='Encoder-Layer1',activation=activations.relu, activity_regularizer=regularizers.l1(1e-3))(visible)\n",
    "e = Dense(units=100, name='Encoder-Layer2',activation=activations.relu)(e)\n",
    "\n",
    "#--- Bottleneck\n",
    "bottleneck = Dense(units=n_bottleneck, name='Bottleneck-Layer')(e)\n",
    "\n",
    "#--- Decoder Layer\n",
    "d = Dense(units=100, name='Decoder-Layer1',activation=activations.relu)(bottleneck)\n",
    "d = Dense(units=200, name='Decoder-Layer2',activation=activations.relu)(d)\n",
    "\n",
    "#--- Output layer\n",
    "output = Dense(units=n_inputs, activation='relu', name='Output-Layer')(d)\n",
    "\n",
    "# Define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output, name='Autoencoder-Model')\n",
    "\n",
    "# Compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train_noisy, X_train_noisy, epochs=30, batch_size=32, verbose=1, validation_data=(X_test_noisy, X_test_noisy))\n",
    "\n",
    "X_encoded_denoise = model.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode_denoise = model.predict(X_train_noisy)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode_denoise = model.predict(X_test_noisy)\n",
    "\n",
    "\n",
    "# Train the random forest model using the encoded features\n",
    "rfr_denoise = RandomForestRegressor()\n",
    "rfr_denoise.fit(X_train_encode_denoise, y_train)\n",
    "\n",
    "#test prediction\n",
    "prediction_denoise = rfr_denoise.predict(X_test_encode_denoise)\n",
    "\n",
    "# Calculate the MAE and MSE\n",
    "mae = mean_absolute_error(y_test, prediction_denoise)\n",
    "mse = mean_squared_error(y_test, prediction_denoise)\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, prediction_denoise)))\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print('Train score:',rfr_denoise.score(X_train_encode_denoise,y_train))\n",
    "print('Test score:',rfr_denoise.score(X_test_encode_denoise,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53afb6-c184-4f23-a394-ae688a641a2a",
   "metadata": {},
   "source": [
    "### Random Forest and Variational Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb868450-346e-4451-8fa4-6341519d0087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 33)]         0           []                               \n",
      "                                                                                                  \n",
      " encoding1 (Dense)              (None, 200)          6800        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " encoding (Dense)               (None, 100)          20100       ['encoding1[0][0]']              \n",
      "                                                                                                  \n",
      " mean (Dense)                   (None, 50)           5050        ['encoding[0][0]']               \n",
      "                                                                                                  \n",
      " log-variance (Dense)           (None, 50)           5050        ['encoding[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 50)           0           ['mean[0][0]',                   \n",
      "                                                                  'log-variance[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37,000\n",
      "Trainable params: 37,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 50)]              0         \n",
      "                                                                 \n",
      " decoder_h2 (Dense)          (None, 100)               5100      \n",
      "                                                                 \n",
      " decoder_h (Dense)           (None, 200)               20200     \n",
      "                                                                 \n",
      " flat_decoded (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,933\n",
      "Trainable params: 31,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 33)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        [(None, 50),              37000     \n",
      "                              (None, 50),                        \n",
      "                              (None, 50)]                        \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 33)                31933     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,933\n",
      "Trainable params: 68,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21055 samples, validate on 7019 samples\n",
      "Epoch 1/30\n",
      "20160/21055 [===========================>..] - ETA: 0s - loss: 12.7463 - mae: 0.3539"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21055/21055 [==============================] - 1s 63us/sample - loss: 12.6108 - mae: 0.3502 - val_loss: 9.8016 - val_mae: 0.2758\n",
      "Epoch 2/30\n",
      "21055/21055 [==============================] - 1s 49us/sample - loss: 9.2863 - mae: 0.2640 - val_loss: 8.9320 - val_mae: 0.2559\n",
      "Epoch 3/30\n",
      "21055/21055 [==============================] - 1s 59us/sample - loss: 8.6997 - mae: 0.2502 - val_loss: 8.5182 - val_mae: 0.2453\n",
      "Epoch 4/30\n",
      "21055/21055 [==============================] - 1s 57us/sample - loss: 8.3305 - mae: 0.2399 - val_loss: 8.1588 - val_mae: 0.2347\n",
      "Epoch 5/30\n",
      "21055/21055 [==============================] - 1s 60us/sample - loss: 8.0290 - mae: 0.2312 - val_loss: 7.8712 - val_mae: 0.2268\n",
      "Epoch 6/30\n",
      "21055/21055 [==============================] - 1s 60us/sample - loss: 7.7761 - mae: 0.2245 - val_loss: 7.7478 - val_mae: 0.2243\n",
      "Epoch 7/30\n",
      "21055/21055 [==============================] - 1s 50us/sample - loss: 7.6129 - mae: 0.2208 - val_loss: 7.5493 - val_mae: 0.2194\n",
      "Epoch 8/30\n",
      "21055/21055 [==============================] - 1s 60us/sample - loss: 7.5153 - mae: 0.2190 - val_loss: 7.4740 - val_mae: 0.2181\n",
      "Epoch 9/30\n",
      "21055/21055 [==============================] - 1s 63us/sample - loss: 7.4373 - mae: 0.2175 - val_loss: 7.3595 - val_mae: 0.2155\n",
      "Epoch 10/30\n",
      "21055/21055 [==============================] - 1s 56us/sample - loss: 7.3636 - mae: 0.2160 - val_loss: 7.3754 - val_mae: 0.2167\n",
      "Epoch 11/30\n",
      "21055/21055 [==============================] - 1s 59us/sample - loss: 7.3202 - mae: 0.2153 - val_loss: 7.3500 - val_mae: 0.2165\n",
      "Epoch 12/30\n",
      "21055/21055 [==============================] - 1s 42us/sample - loss: 7.2708 - mae: 0.2143 - val_loss: 7.2523 - val_mae: 0.2140\n",
      "Epoch 13/30\n",
      "21055/21055 [==============================] - 1s 39us/sample - loss: 7.2482 - mae: 0.2140 - val_loss: 7.2442 - val_mae: 0.2141\n",
      "Epoch 14/30\n",
      "21055/21055 [==============================] - 1s 41us/sample - loss: 7.2196 - mae: 0.2135 - val_loss: 7.1939 - val_mae: 0.2129\n",
      "Epoch 15/30\n",
      "21055/21055 [==============================] - 1s 61us/sample - loss: 7.1855 - mae: 0.2128 - val_loss: 7.2450 - val_mae: 0.2147\n",
      "Epoch 16/30\n",
      "21055/21055 [==============================] - 2s 76us/sample - loss: 7.1891 - mae: 0.2131 - val_loss: 7.1772 - val_mae: 0.2129\n",
      "Epoch 17/30\n",
      "21055/21055 [==============================] - 1s 66us/sample - loss: 7.1645 - mae: 0.2126 - val_loss: 7.1305 - val_mae: 0.2116\n",
      "Epoch 18/30\n",
      "21055/21055 [==============================] - 1s 69us/sample - loss: 7.1505 - mae: 0.2124 - val_loss: 7.1881 - val_mae: 0.2136\n",
      "Epoch 19/30\n",
      "21055/21055 [==============================] - 1s 59us/sample - loss: 7.1306 - mae: 0.2120 - val_loss: 7.1468 - val_mae: 0.2125\n",
      "Epoch 20/30\n",
      "21055/21055 [==============================] - 1s 55us/sample - loss: 7.1215 - mae: 0.2118 - val_loss: 7.1521 - val_mae: 0.2128\n",
      "Epoch 21/30\n",
      "21055/21055 [==============================] - 1s 55us/sample - loss: 7.1041 - mae: 0.2114 - val_loss: 7.1569 - val_mae: 0.2131\n",
      "Epoch 22/30\n",
      "21055/21055 [==============================] - 1s 59us/sample - loss: 7.1003 - mae: 0.2115 - val_loss: 7.1548 - val_mae: 0.2131\n",
      "Epoch 23/30\n",
      "21055/21055 [==============================] - 1s 52us/sample - loss: 7.1002 - mae: 0.2116 - val_loss: 7.1088 - val_mae: 0.2119\n",
      "Epoch 24/30\n",
      "21055/21055 [==============================] - 1s 60us/sample - loss: 7.0845 - mae: 0.2112 - val_loss: 7.0859 - val_mae: 0.2113\n",
      "Epoch 25/30\n",
      "21055/21055 [==============================] - 1s 47us/sample - loss: 7.0698 - mae: 0.2109 - val_loss: 7.1242 - val_mae: 0.2126\n",
      "Epoch 26/30\n",
      "21055/21055 [==============================] - 1s 43us/sample - loss: 7.0601 - mae: 0.2107 - val_loss: 7.1028 - val_mae: 0.2120\n",
      "Epoch 27/30\n",
      "21055/21055 [==============================] - 1s 55us/sample - loss: 7.0529 - mae: 0.2105 - val_loss: 7.0321 - val_mae: 0.2099\n",
      "Epoch 28/30\n",
      "21055/21055 [==============================] - 1s 62us/sample - loss: 7.0361 - mae: 0.2101 - val_loss: 7.0580 - val_mae: 0.2108\n",
      "Epoch 29/30\n",
      "21055/21055 [==============================] - 1s 54us/sample - loss: 7.0367 - mae: 0.2102 - val_loss: 7.0880 - val_mae: 0.2118\n",
      "Epoch 30/30\n",
      "21055/21055 [==============================] - 1s 56us/sample - loss: 7.0417 - mae: 0.2104 - val_loss: 7.1079 - val_mae: 0.2124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.6375983816259955\n",
      "MSE: 1.0461761983753184\n",
      "RMSE: 1.022827550653246\n",
      "Train score: 0.9809442238069822\n",
      "Test score: 0.860161960049641\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "original_dim = (X_train.shape[1])\n",
    "latent_dim = 50\n",
    "intermediate_dim1 = 200\n",
    "intermediate_dim2 = 100\n",
    "epochs = 30\n",
    "epsilon_std = 0.000001\n",
    "\n",
    "\n",
    "# sampling from mean and sd in VAE\n",
    "def sampling(args: tuple):\n",
    "    # we grab the variables from the tuple\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "#########################\n",
    "# input to our encoder\n",
    "#########################\n",
    "x = Input(shape=(original_dim,), name=\"input\")\n",
    "\n",
    "# intermediate layer\n",
    "#h1 = Dense(intermediate_dim1, activation='tanh', name=\"encoding1\")(x)\n",
    "h1 = Dense(intermediate_dim1, activation=\"tanh\", name=\"encoding1\", activity_regularizer=regularizers.l1(10e-5))(x)\n",
    "h = Dense(intermediate_dim2, activation='tanh', name=\"encoding\")(h1)\n",
    "\n",
    "\n",
    "# defining the mean of the latent space\n",
    "z_mean = Dense(latent_dim, name=\"mean\")(h)\n",
    "\n",
    "# defining the log variance of the latent space\n",
    "z_log_var = Dense(latent_dim, name=\"log-variance\")(h)\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# defining the encoder as a keras model\n",
    "encoder = Model(x, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# print out summary of what we just did\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "#########################\n",
    "# Input to the decoder\n",
    "#########################\n",
    "\n",
    "input_decoder = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
    "\n",
    "# taking the latent space to intermediate dimension\n",
    "decoder_h1 = Dense(intermediate_dim2, activation='relu', name=\"decoder_h2\")(input_decoder)\n",
    "decoder_h = Dense(intermediate_dim1, activation='relu', name=\"decoder_h\")(decoder_h1)\n",
    "\n",
    "# getting the mean from the original dimension\n",
    "x_decoded = Dense(original_dim, activation='tanh', name=\"flat_decoded\")(decoder_h)\n",
    "\n",
    "# defining the decoder as a keras model\n",
    "decoder = Model(input_decoder, x_decoded, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "##########################\n",
    "# Variational Autoencoder\n",
    "##########################\n",
    "\n",
    "# grab the output. Recall, that we need to grab the 3rd element our sampling z\n",
    "output_combined = decoder(encoder(x)[2])\n",
    "\n",
    "# link the input and the overall output\n",
    "vae = Model(x, output_combined)\n",
    "\n",
    "# print out what the overall model looks like\n",
    "vae.summary()\n",
    "\n",
    "# Defina VAE Loss Function\n",
    "def vae_loss(x: tf.Tensor, x_decoded_mean: tf.Tensor,z_log_var=z_log_var, z_mean=z_mean, original_dim=original_dim):\n",
    "    xent_loss = original_dim * metrics.mae(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss, metrics=['mae'],experimental_run_tf_function=False)\n",
    "\n",
    "history = vae.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1).history\n",
    "\n",
    "\n",
    "X_encoded = vae.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode_vae = vae.predict(X_train)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode_vae = vae.predict(X_test)\n",
    "\n",
    "# Train the random forest model using the encoded features\n",
    "rfr_vae = RandomForestRegressor()\n",
    "rfr_vae.fit(X_train_encode_vae, y_train)\n",
    "\n",
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2))\n",
    "    #return MAE\n",
    "    return np.mean(abs(v1 - v2))\n",
    "\n",
    "X_train_pred = rfr_vae.predict(X_train_encode_vae)\n",
    "mae_vector_train = get_error_term(X_train_pred, y_train, _rmse=False)\n",
    "                   \n",
    "X_pred = rfr_vae.predict(X_test_encode_vae)\n",
    "mae_vector_test = get_error_term(X_pred, y_test, _rmse=False)\n",
    "                   \n",
    "\n",
    "#metrics \n",
    "print('MAE:', mean_absolute_error(y_test,X_pred))\n",
    "print('MSE:', mean_squared_error(y_test, X_pred))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, X_pred)))\n",
    "print('Train score:',rfr_vae.score(X_train_encode_vae,y_train))\n",
    "print('Test score:',rfr_vae.score(X_test_encode_vae,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6518f65-7310-4fa7-90ee-bc7075db9d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
