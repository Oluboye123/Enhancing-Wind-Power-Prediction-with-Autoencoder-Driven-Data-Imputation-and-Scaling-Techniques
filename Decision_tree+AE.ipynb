{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f969911d-66c2-4e7d-8246-8da98af9b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Model, load_model # for creating a Neural Network Autoencoder model\n",
    "from keras import Input # for instantiating a keras tensor\n",
    "from keras.layers import Dense # for adding layers to AE model\n",
    "from tensorflow.keras.utils import plot_model #for plotting  model charts\n",
    "from tensorflow.keras import models,layers,activations,losses,optimizers,metrics\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split,cross_validate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, TransformerMixin\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d41af81-3f38-452f-8e42-ae312b613b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#oad data\n",
    "train_data = pd.read_pickle(\"EDA_train_median.pkl\")\n",
    "test_data = pd.read_pickle(\"EDA_test_median.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c889f4bd-6ce2-4caa-ba6c-2dc1e4fa5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate target variable\n",
    "y = train_data['windmill_generated_power(kW/h)']\n",
    "train_data.drop(['windmill_generated_power(kW/h)'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f306715-2f00-485f-80e2-27b5eee655ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your data\n",
    "scaler.fit(train_data)\n",
    "\n",
    "# Transform the data using the scaler\n",
    "scaled_data = scaler.transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e8be9f-d692-49d2-8568-66526d93d248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21051, 33), (21051,), (7018, 33), (7018,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split dataset \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, random_state=0)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0f3f2a-a055-46f9-8acc-43363ec1a55e",
   "metadata": {},
   "source": [
    "### GENERAL AUTOENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04a0a64e-e5d1-4d53-9367-66237986e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Autoencoder-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (InputLayer)    [(None, 33)]              0         \n",
      "                                                                 \n",
      " Encoder-Layer1 (Dense)      (None, 200)               6800      \n",
      "                                                                 \n",
      " Encoder-Layer2 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Bottleneck-Layer (Dense)    (None, 50)                5050      \n",
      "                                                                 \n",
      " Decoder-Layer1 (Dense)      (None, 100)               5100      \n",
      "                                                                 \n",
      " Decoder-Layer2 (Dense)      (None, 200)               20200     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,883\n",
      "Trainable params: 63,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21051 samples, validate on 7018 samples\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21051/21051 - 2s - loss: 0.5519 - val_loss: 0.5322 - 2s/epoch - 110us/sample\n",
      "Epoch 2/30\n",
      "21051/21051 - 2s - loss: 0.5261 - val_loss: 0.5241 - 2s/epoch - 102us/sample\n",
      "Epoch 3/30\n",
      "21051/21051 - 2s - loss: 0.5211 - val_loss: 0.5207 - 2s/epoch - 95us/sample\n",
      "Epoch 4/30\n",
      "21051/21051 - 2s - loss: 0.5191 - val_loss: 0.5187 - 2s/epoch - 102us/sample\n",
      "Epoch 5/30\n",
      "21051/21051 - 2s - loss: 0.5181 - val_loss: 0.5188 - 2s/epoch - 97us/sample\n",
      "Epoch 6/30\n",
      "21051/21051 - 2s - loss: 0.5176 - val_loss: 0.5183 - 2s/epoch - 90us/sample\n",
      "Epoch 7/30\n",
      "21051/21051 - 2s - loss: 0.5172 - val_loss: 0.5187 - 2s/epoch - 103us/sample\n",
      "Epoch 8/30\n",
      "21051/21051 - 2s - loss: 0.5171 - val_loss: 0.5176 - 2s/epoch - 100us/sample\n",
      "Epoch 9/30\n",
      "21051/21051 - 2s - loss: 0.5169 - val_loss: 0.5176 - 2s/epoch - 103us/sample\n",
      "Epoch 10/30\n",
      "21051/21051 - 2s - loss: 0.5167 - val_loss: 0.5179 - 2s/epoch - 99us/sample\n",
      "Epoch 11/30\n",
      "21051/21051 - 2s - loss: 0.5167 - val_loss: 0.5173 - 2s/epoch - 110us/sample\n",
      "Epoch 12/30\n",
      "21051/21051 - 2s - loss: 0.5166 - val_loss: 0.5168 - 2s/epoch - 105us/sample\n",
      "Epoch 13/30\n",
      "21051/21051 - 2s - loss: 0.5166 - val_loss: 0.5172 - 2s/epoch - 103us/sample\n",
      "Epoch 14/30\n",
      "21051/21051 - 2s - loss: 0.5165 - val_loss: 0.5172 - 2s/epoch - 90us/sample\n",
      "Epoch 15/30\n",
      "21051/21051 - 2s - loss: 0.5164 - val_loss: 0.5179 - 2s/epoch - 103us/sample\n",
      "Epoch 16/30\n",
      "21051/21051 - 2s - loss: 0.5164 - val_loss: 0.5170 - 2s/epoch - 97us/sample\n",
      "Epoch 17/30\n",
      "21051/21051 - 2s - loss: 0.5163 - val_loss: 0.5172 - 2s/epoch - 96us/sample\n",
      "Epoch 18/30\n",
      "21051/21051 - 2s - loss: 0.5162 - val_loss: 0.5167 - 2s/epoch - 101us/sample\n",
      "Epoch 19/30\n",
      "21051/21051 - 2s - loss: 0.5162 - val_loss: 0.5172 - 2s/epoch - 110us/sample\n",
      "Epoch 20/30\n",
      "21051/21051 - 2s - loss: 0.5162 - val_loss: 0.5171 - 2s/epoch - 98us/sample\n",
      "Epoch 21/30\n",
      "21051/21051 - 2s - loss: 0.5162 - val_loss: 0.5177 - 2s/epoch - 92us/sample\n",
      "Epoch 22/30\n",
      "21051/21051 - 2s - loss: 0.5163 - val_loss: 0.5169 - 2s/epoch - 88us/sample\n",
      "Epoch 23/30\n",
      "21051/21051 - 2s - loss: 0.5161 - val_loss: 0.5170 - 2s/epoch - 101us/sample\n",
      "Epoch 24/30\n",
      "21051/21051 - 2s - loss: 0.5160 - val_loss: 0.5168 - 2s/epoch - 93us/sample\n",
      "Epoch 25/30\n",
      "21051/21051 - 2s - loss: 0.5161 - val_loss: 0.5174 - 2s/epoch - 104us/sample\n",
      "Epoch 26/30\n",
      "21051/21051 - 2s - loss: 0.5161 - val_loss: 0.5171 - 2s/epoch - 108us/sample\n",
      "Epoch 27/30\n",
      "21051/21051 - 2s - loss: 0.5159 - val_loss: 0.5177 - 2s/epoch - 102us/sample\n",
      "Epoch 28/30\n",
      "21051/21051 - 2s - loss: 0.5159 - val_loss: 0.5168 - 2s/epoch - 103us/sample\n",
      "Epoch 29/30\n",
      "21051/21051 - 2s - loss: 0.5158 - val_loss: 0.5173 - 2s/epoch - 96us/sample\n",
      "Epoch 30/30\n",
      "21051/21051 - 2s - loss: 0.5158 - val_loss: 0.5171 - 2s/epoch - 114us/sample\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff5e1e6b20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1] # number of input neurons = the number of features in X_train\n",
    "n_bottleneck= 50\n",
    "\n",
    "#--- Input Layer \n",
    "visible = Input(shape=(n_inputs,), name='Input-Layer') # Specify input shape\n",
    "\n",
    "#--- Encoder Layer\n",
    "e = Dense(units=200, name='Encoder-Layer1',activation=activations.relu, activity_regularizer=regularizers.l1(1e-3))(visible)\n",
    "e = Dense(units=100, name='Encoder-Layer2',activation=activations.relu)(e)\n",
    "\n",
    "#--- Bottleneck\n",
    "bottleneck = Dense(units=n_bottleneck, name='Bottleneck-Layer')(e)\n",
    "\n",
    "#--- Decoder Layer\n",
    "d = Dense(units=100, name='Decoder-Layer1',activation=activations.relu)(bottleneck)\n",
    "d = Dense(units=200, name='Decoder-Layer2',activation=activations.relu)(d)\n",
    "\n",
    "#--- Output layer\n",
    "output = Dense(units=n_inputs, activation='relu', name='Output-Layer')(d)\n",
    "\n",
    "# Define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output, name='Autoencoder-Model')\n",
    "\n",
    "# Compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Train the autoencoder model\n",
    "model.fit(X_train, X_train, epochs=30, batch_size=16, verbose=2, validation_data=(X_test, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bc5857-6fa7-4fd5-9b2a-bb3246f67728",
   "metadata": {},
   "source": [
    "### Decision Tree and General Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95afc901-f2a5-4de3-8e2b-a83199927ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 1.157587516757572\n",
      "MSE: 2.5321592688804175\n",
      "RMSE: 1.5912759876528073\n",
      "Train score: 0.7148905503621141\n",
      "Test score: 0.6559349384472315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_encoded = model.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode = model.predict(X_train)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode = model.predict(X_test)\n",
    "\n",
    "# Train the decision tree model using the encoded features\n",
    "dtr = DecisionTreeRegressor(max_depth = 30 , max_features = 'auto', min_samples_leaf = 30, min_samples_split = 20)\n",
    "dtr.fit(X_train_encode, y_train)\n",
    "\n",
    "\n",
    "#reshape test data\n",
    "#X_test_encode_new = X_test_encode.reshape(-1, 1)\n",
    "#y_test_new = y_test.to_numpy().reshape(-1, 1)\n",
    "\n",
    "# prediction \n",
    "pred = dtr.predict(X_test_encode)\n",
    "\n",
    "#metrics \n",
    "print('MAE:', mean_absolute_error(y_test,pred))\n",
    "print('MSE:', mean_squared_error(y_test, pred))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, pred)))\n",
    "print('Train score:',dtr.score(X_train_encode,y_train))\n",
    "print('Test score:',dtr.score(X_test_encode,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661fe9d-1fde-457e-9292-d400a8c4fde0",
   "metadata": {},
   "source": [
    "### Decision tree and Denoising Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "533e2f20-9929-4b34-8b7f-e924bcdd8992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New shape of X_train:  (21051, 33)\n",
      "New shape of X_test:  (7018, 33)\n",
      "New shape of X_train_noisy:  (21051, 33)\n",
      "New shape of X_test_noisy:  (7018, 33)\n",
      "Model: \"Autoencoder-Model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input-Layer (InputLayer)    [(None, 33)]              0         \n",
      "                                                                 \n",
      " Encoder-Layer1 (Dense)      (None, 200)               6800      \n",
      "                                                                 \n",
      " Encoder-Layer2 (Dense)      (None, 100)               20100     \n",
      "                                                                 \n",
      " Bottleneck-Layer (Dense)    (None, 50)                5050      \n",
      "                                                                 \n",
      " Decoder-Layer1 (Dense)      (None, 100)               5100      \n",
      "                                                                 \n",
      " Decoder-Layer2 (Dense)      (None, 200)               20200     \n",
      "                                                                 \n",
      " Output-Layer (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 63,883\n",
      "Trainable params: 63,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 21051 samples, validate on 7018 samples\n",
      "Epoch 1/30\n",
      "20544/21051 [============================>.] - ETA: 0s - loss: 0.1063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21051/21051 [==============================] - 2s 92us/sample - loss: 0.1061 - val_loss: 0.0937\n",
      "Epoch 2/30\n",
      "21051/21051 [==============================] - 2s 72us/sample - loss: 0.0919 - val_loss: 0.0903\n",
      "Epoch 3/30\n",
      "21051/21051 [==============================] - 1s 70us/sample - loss: 0.0893 - val_loss: 0.0880\n",
      "Epoch 4/30\n",
      "21051/21051 [==============================] - 2s 75us/sample - loss: 0.0880 - val_loss: 0.0872\n",
      "Epoch 5/30\n",
      "21051/21051 [==============================] - 1s 70us/sample - loss: 0.0874 - val_loss: 0.0867\n",
      "Epoch 6/30\n",
      "21051/21051 [==============================] - 1s 67us/sample - loss: 0.0869 - val_loss: 0.0867\n",
      "Epoch 7/30\n",
      "21051/21051 [==============================] - 2s 76us/sample - loss: 0.0867 - val_loss: 0.0864\n",
      "Epoch 8/30\n",
      "21051/21051 [==============================] - 2s 76us/sample - loss: 0.0865 - val_loss: 0.0860\n",
      "Epoch 9/30\n",
      "21051/21051 [==============================] - 2s 77us/sample - loss: 0.0863 - val_loss: 0.0861\n",
      "Epoch 10/30\n",
      "21051/21051 [==============================] - 2s 73us/sample - loss: 0.0862 - val_loss: 0.0860\n",
      "Epoch 11/30\n",
      "21051/21051 [==============================] - 2s 73us/sample - loss: 0.0861 - val_loss: 0.0855\n",
      "Epoch 12/30\n",
      "21051/21051 [==============================] - 2s 74us/sample - loss: 0.0860 - val_loss: 0.0860\n",
      "Epoch 13/30\n",
      "21051/21051 [==============================] - 1s 69us/sample - loss: 0.0860 - val_loss: 0.0859\n",
      "Epoch 14/30\n",
      "21051/21051 [==============================] - 1s 71us/sample - loss: 0.0860 - val_loss: 0.0857\n",
      "Epoch 15/30\n",
      "21051/21051 [==============================] - 1s 69us/sample - loss: 0.0857 - val_loss: 0.0858\n",
      "Epoch 16/30\n",
      "21051/21051 [==============================] - 1s 69us/sample - loss: 0.0857 - val_loss: 0.0860\n",
      "Epoch 17/30\n",
      "21051/21051 [==============================] - 2s 72us/sample - loss: 0.0856 - val_loss: 0.0848\n",
      "Epoch 18/30\n",
      "21051/21051 [==============================] - 1s 69us/sample - loss: 0.0856 - val_loss: 0.0856\n",
      "Epoch 19/30\n",
      "21051/21051 [==============================] - 1s 67us/sample - loss: 0.0856 - val_loss: 0.0850\n",
      "Epoch 20/30\n",
      "21051/21051 [==============================] - 2s 72us/sample - loss: 0.0855 - val_loss: 0.0854\n",
      "Epoch 21/30\n",
      "21051/21051 [==============================] - 2s 76us/sample - loss: 0.0853 - val_loss: 0.0856\n",
      "Epoch 22/30\n",
      "21051/21051 [==============================] - 2s 79us/sample - loss: 0.0854 - val_loss: 0.0852\n",
      "Epoch 23/30\n",
      "21051/21051 [==============================] - 2s 76us/sample - loss: 0.0854 - val_loss: 0.0853\n",
      "Epoch 24/30\n",
      "21051/21051 [==============================] - 1s 68us/sample - loss: 0.0853 - val_loss: 0.0851\n",
      "Epoch 25/30\n",
      "21051/21051 [==============================] - 1s 67us/sample - loss: 0.0852 - val_loss: 0.0846\n",
      "Epoch 26/30\n",
      "21051/21051 [==============================] - 1s 70us/sample - loss: 0.0853 - val_loss: 0.0848\n",
      "Epoch 27/30\n",
      "21051/21051 [==============================] - 1s 65us/sample - loss: 0.0853 - val_loss: 0.0851\n",
      "Epoch 28/30\n",
      "21051/21051 [==============================] - 1s 71us/sample - loss: 0.0852 - val_loss: 0.0849\n",
      "Epoch 29/30\n",
      "21051/21051 [==============================] - 2s 72us/sample - loss: 0.0851 - val_loss: 0.0848\n",
      "Epoch 30/30\n",
      "21051/21051 [==============================] - 1s 71us/sample - loss: 0.0853 - val_loss: 0.0846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.7479312845218236\n",
      "Mean Absolute Error: 1.279814523847353\n",
      "Mean Squared Error: 3.0552637754101117\n",
      "Train score: 0.6668375802482119\n",
      "Test score: 0.5848564772897513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Specify how much noise to add\n",
    "level_of_noise = 0.00001\n",
    "\n",
    "# Add random noise based on sampling from Gaussian distribution\n",
    "X_train_noisy = X_train + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)\n",
    "X_test_noisy = X_test + level_of_noise * np.random.normal(loc=0.0, scale=1.0, size=X_test.shape)\n",
    "\n",
    "# Enforce min-max boundaries so it does not go beyond [0,1] range\n",
    "X_train_noisy = np.clip(X_train_noisy, 0., 1.)\n",
    "X_test_noisy = np.clip(X_test_noisy, 0., 1.)\n",
    "# Print shapes\n",
    "print(\"New shape of X_train: \", X_train.shape)\n",
    "print(\"New shape of X_test: \", X_test.shape)\n",
    "\n",
    "print(\"New shape of X_train_noisy: \", X_train_noisy.shape)\n",
    "print(\"New shape of X_test_noisy: \", X_test_noisy.shape)\n",
    "\n",
    "\n",
    "#--- Define Shapes\n",
    "n_inputs = X_train_noisy.shape[1] # number of input neurons = the number of features in X_train\n",
    "n_bottleneck= 50\n",
    "\n",
    "#--- Input Layer \n",
    "visible = Input(shape=(n_inputs,), name='Input-Layer') # Specify input shape\n",
    "\n",
    "#--- Encoder Layer\n",
    "e = Dense(units=200, name='Encoder-Layer1',activation=activations.relu, activity_regularizer=regularizers.l1(1e-3))(visible)\n",
    "e = Dense(units=100, name='Encoder-Layer2',activation=activations.relu)(e)\n",
    "\n",
    "#--- Bottleneck\n",
    "bottleneck = Dense(units=n_bottleneck, name='Bottleneck-Layer')(e)\n",
    "\n",
    "#--- Decoder Layer\n",
    "d = Dense(units=100, name='Decoder-Layer1',activation=activations.relu)(bottleneck)\n",
    "d = Dense(units=200, name='Decoder-Layer2',activation=activations.relu)(d)\n",
    "\n",
    "#--- Output layer\n",
    "output = Dense(units=n_inputs, activation='relu', name='Output-Layer')(d)\n",
    "\n",
    "# Define autoencoder model\n",
    "model = Model(inputs=visible, outputs=output, name='Autoencoder-Model')\n",
    "\n",
    "# Compile autoencoder model\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Print model summary\n",
    "print(model.summary())\n",
    "\n",
    "# Fit the autoencoder model to reconstruct input\n",
    "history = model.fit(X_train_noisy, X_train_noisy, epochs=30, batch_size=32, verbose=1, validation_data=(X_test_noisy, X_test_noisy))\n",
    "\n",
    "X_encoded_denoise = model.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode_denoise = model.predict(X_train_noisy)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode_denoise = model.predict(X_test_noisy)\n",
    "\n",
    "# Train the decision tree model using the encoded features\n",
    "dtr_denoise = DecisionTreeRegressor(max_depth = 30 , max_features = 'auto', min_samples_leaf = 30, min_samples_split = 20)\n",
    "dtr_denoise.fit(X_train_encode_denoise, y_train)\n",
    "\n",
    "#test prediction\n",
    "prediction_denoise = dtr_denoise.predict(X_test_encode_denoise)\n",
    "\n",
    "# Calculate the MAE and MSE\n",
    "mae = mean_absolute_error(y_test, prediction_denoise)\n",
    "mse = mean_squared_error(y_test, prediction_denoise)\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, prediction_denoise)))\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print('Train score:',dtr_denoise.score(X_train_encode_denoise,y_train))\n",
    "print('Test score:',dtr_denoise.score(X_test_encode_denoise,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d53afb6-c184-4f23-a394-ae688a641a2a",
   "metadata": {},
   "source": [
    "### Decision tree and Variational Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb868450-346e-4451-8fa4-6341519d0087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 33)]         0           []                               \n",
      "                                                                                                  \n",
      " encoding1 (Dense)              (None, 200)          6800        ['input[0][0]']                  \n",
      "                                                                                                  \n",
      " encoding (Dense)               (None, 100)          20100       ['encoding1[0][0]']              \n",
      "                                                                                                  \n",
      " mean (Dense)                   (None, 50)           5050        ['encoding[0][0]']               \n",
      "                                                                                                  \n",
      " log-variance (Dense)           (None, 50)           5050        ['encoding[0][0]']               \n",
      "                                                                                                  \n",
      " lambda_1 (Lambda)              (None, 50)           0           ['mean[0][0]',                   \n",
      "                                                                  'log-variance[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 37,000\n",
      "Trainable params: 37,000\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " decoder_input (InputLayer)  [(None, 50)]              0         \n",
      "                                                                 \n",
      " decoder_h2 (Dense)          (None, 100)               5100      \n",
      "                                                                 \n",
      " decoder_h (Dense)           (None, 200)               20200     \n",
      "                                                                 \n",
      " flat_decoded (Dense)        (None, 33)                6633      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 31,933\n",
      "Trainable params: 31,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 33)]              0         \n",
      "                                                                 \n",
      " encoder (Functional)        [(None, 50),              37000     \n",
      "                              (None, 50),                        \n",
      "                              (None, 50)]                        \n",
      "                                                                 \n",
      " decoder (Functional)        (None, 33)                31933     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,933\n",
      "Trainable params: 68,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 21051 samples, validate on 7018 samples\n",
      "Epoch 1/30\n",
      "20480/21051 [============================>.] - ETA: 0s - loss: 12.6300 - mae: 0.3523"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21051/21051 [==============================] - 2s 72us/sample - loss: 12.5454 - mae: 0.3501 - val_loss: 9.7701 - val_mae: 0.2755\n",
      "Epoch 2/30\n",
      "21051/21051 [==============================] - 1s 59us/sample - loss: 9.2958 - mae: 0.2649 - val_loss: 8.9579 - val_mae: 0.2573\n",
      "Epoch 3/30\n",
      "21051/21051 [==============================] - 1s 53us/sample - loss: 8.7484 - mae: 0.2524 - val_loss: 8.5683 - val_mae: 0.2477\n",
      "Epoch 4/30\n",
      "21051/21051 [==============================] - 1s 54us/sample - loss: 8.4212 - mae: 0.2436 - val_loss: 8.3083 - val_mae: 0.2404\n",
      "Epoch 5/30\n",
      "21051/21051 [==============================] - 1s 56us/sample - loss: 8.1483 - mae: 0.2352 - val_loss: 8.0122 - val_mae: 0.2309\n",
      "Epoch 6/30\n",
      "21051/21051 [==============================] - 1s 55us/sample - loss: 7.9034 - mae: 0.2279 - val_loss: 7.8259 - val_mae: 0.2261\n",
      "Epoch 7/30\n",
      "21051/21051 [==============================] - 1s 52us/sample - loss: 7.7265 - mae: 0.2235 - val_loss: 7.6456 - val_mae: 0.2217\n",
      "Epoch 8/30\n",
      "21051/21051 [==============================] - 1s 53us/sample - loss: 7.5913 - mae: 0.2204 - val_loss: 7.5485 - val_mae: 0.2197\n",
      "Epoch 9/30\n",
      "21051/21051 [==============================] - 1s 54us/sample - loss: 7.4996 - mae: 0.2186 - val_loss: 7.4288 - val_mae: 0.2169\n",
      "Epoch 10/30\n",
      "21051/21051 [==============================] - 1s 50us/sample - loss: 7.4288 - mae: 0.2173 - val_loss: 7.3523 - val_mae: 0.2153\n",
      "Epoch 11/30\n",
      "21051/21051 [==============================] - 1s 58us/sample - loss: 7.3654 - mae: 0.2160 - val_loss: 7.3256 - val_mae: 0.2151\n",
      "Epoch 12/30\n",
      "21051/21051 [==============================] - 1s 56us/sample - loss: 7.3280 - mae: 0.2154 - val_loss: 7.3240 - val_mae: 0.2155\n",
      "Epoch 13/30\n",
      "21051/21051 [==============================] - 1s 58us/sample - loss: 7.2901 - mae: 0.2147 - val_loss: 7.3233 - val_mae: 0.2159\n",
      "Epoch 14/30\n",
      "21051/21051 [==============================] - 1s 49us/sample - loss: 7.2685 - mae: 0.2144 - val_loss: 7.2860 - val_mae: 0.2151\n",
      "Epoch 15/30\n",
      "21051/21051 [==============================] - 1s 54us/sample - loss: 7.2595 - mae: 0.2145 - val_loss: 7.1949 - val_mae: 0.2127\n",
      "Epoch 16/30\n",
      "21051/21051 [==============================] - 1s 56us/sample - loss: 7.2360 - mae: 0.2141 - val_loss: 7.2697 - val_mae: 0.2152\n",
      "Epoch 17/30\n",
      "21051/21051 [==============================] - 1s 54us/sample - loss: 7.2167 - mae: 0.2137 - val_loss: 7.1476 - val_mae: 0.2117\n",
      "Epoch 18/30\n",
      "21051/21051 [==============================] - 1s 56us/sample - loss: 7.1994 - mae: 0.2134 - val_loss: 7.1285 - val_mae: 0.2113\n",
      "Epoch 19/30\n",
      "21051/21051 [==============================] - 1s 59us/sample - loss: 7.1845 - mae: 0.2131 - val_loss: 7.2219 - val_mae: 0.2144\n",
      "Epoch 20/30\n",
      "21051/21051 [==============================] - 1s 62us/sample - loss: 7.1528 - mae: 0.2123 - val_loss: 7.1165 - val_mae: 0.2113\n",
      "Epoch 21/30\n",
      "21051/21051 [==============================] - 1s 53us/sample - loss: 7.1462 - mae: 0.2123 - val_loss: 7.1360 - val_mae: 0.2121\n",
      "Epoch 22/30\n",
      "21051/21051 [==============================] - 1s 56us/sample - loss: 7.1224 - mae: 0.2118 - val_loss: 7.1283 - val_mae: 0.2120\n",
      "Epoch 23/30\n",
      "21051/21051 [==============================] - 1s 51us/sample - loss: 7.1047 - mae: 0.2113 - val_loss: 7.0913 - val_mae: 0.2110\n",
      "Epoch 24/30\n",
      "21051/21051 [==============================] - 1s 49us/sample - loss: 7.0991 - mae: 0.2113 - val_loss: 7.1009 - val_mae: 0.2114\n",
      "Epoch 25/30\n",
      "21051/21051 [==============================] - 1s 52us/sample - loss: 7.0942 - mae: 0.2113 - val_loss: 7.1008 - val_mae: 0.2116\n",
      "Epoch 26/30\n",
      "21051/21051 [==============================] - 1s 55us/sample - loss: 7.0750 - mae: 0.2108 - val_loss: 7.0404 - val_mae: 0.2098\n",
      "Epoch 27/30\n",
      "21051/21051 [==============================] - 1s 51us/sample - loss: 7.0657 - mae: 0.2106 - val_loss: 7.0625 - val_mae: 0.2105\n",
      "Epoch 28/30\n",
      "21051/21051 [==============================] - 1s 50us/sample - loss: 7.0536 - mae: 0.2103 - val_loss: 7.0841 - val_mae: 0.2113\n",
      "Epoch 29/30\n",
      "21051/21051 [==============================] - 1s 52us/sample - loss: 7.0449 - mae: 0.2101 - val_loss: 7.0249 - val_mae: 0.2096\n",
      "Epoch 30/30\n",
      "21051/21051 [==============================] - 1s 51us/sample - loss: 7.0475 - mae: 0.2103 - val_loss: 7.0240 - val_mae: 0.2096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\keras\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n",
      "C:\\Users\\Home\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.7832773348978251\n",
      "MSE: 1.4896315560310853\n",
      "RMSE: 1.2205046317122623\n",
      "Train score: 0.8563558604283825\n",
      "Test score: 0.7975916525806077\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "original_dim = (X_train.shape[1])\n",
    "latent_dim = 50\n",
    "intermediate_dim1 = 200\n",
    "intermediate_dim2 = 100\n",
    "epochs = 30\n",
    "epsilon_std = 0.0001\n",
    "\n",
    "\n",
    "# sampling from mean and sd in VAE\n",
    "def sampling(args: tuple):\n",
    "    # we grab the variables from the tuple\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "#########################\n",
    "# input to our encoder\n",
    "#########################\n",
    "x = Input(shape=(original_dim,), name=\"input\")\n",
    "\n",
    "# intermediate layer\n",
    "#h1 = Dense(intermediate_dim1, activation='tanh', name=\"encoding1\")(x)\n",
    "h1 = Dense(intermediate_dim1, activation=\"tanh\", name=\"encoding1\", activity_regularizer=regularizers.l1(10e-5))(x)\n",
    "h = Dense(intermediate_dim2, activation='tanh', name=\"encoding\")(h1)\n",
    "\n",
    "\n",
    "# defining the mean of the latent space\n",
    "z_mean = Dense(latent_dim, name=\"mean\")(h)\n",
    "\n",
    "# defining the log variance of the latent space\n",
    "z_log_var = Dense(latent_dim, name=\"log-variance\")(h)\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "# defining the encoder as a keras model\n",
    "encoder = Model(x, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "# print out summary of what we just did\n",
    "encoder.summary()\n",
    "\n",
    "\n",
    "#########################\n",
    "# Input to the decoder\n",
    "#########################\n",
    "\n",
    "input_decoder = Input(shape=(latent_dim,), name=\"decoder_input\")\n",
    "\n",
    "# taking the latent space to intermediate dimension\n",
    "decoder_h1 = Dense(intermediate_dim2, activation='relu', name=\"decoder_h2\")(input_decoder)\n",
    "decoder_h = Dense(intermediate_dim1, activation='relu', name=\"decoder_h\")(decoder_h1)\n",
    "\n",
    "# getting the mean from the original dimension\n",
    "x_decoded = Dense(original_dim, activation='tanh', name=\"flat_decoded\")(decoder_h)\n",
    "\n",
    "# defining the decoder as a keras model\n",
    "decoder = Model(input_decoder, x_decoded, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "##########################\n",
    "# Variational Autoencoder\n",
    "##########################\n",
    "\n",
    "# grab the output. Recall, that we need to grab the 3rd element our sampling z\n",
    "output_combined = decoder(encoder(x)[2])\n",
    "\n",
    "# link the input and the overall output\n",
    "vae = Model(x, output_combined)\n",
    "\n",
    "# print out what the overall model looks like\n",
    "vae.summary()\n",
    "\n",
    "# Defina VAE Loss Function\n",
    "def vae_loss(x: tf.Tensor, x_decoded_mean: tf.Tensor,z_log_var=z_log_var, z_mean=z_mean, original_dim=original_dim):\n",
    "    xent_loss = original_dim * metrics.mae(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "    return vae_loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss, metrics=['mae'],experimental_run_tf_function=False)\n",
    "\n",
    "history = vae.fit(X_train, X_train,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test, X_test),\n",
    "                    verbose=1).history\n",
    "\n",
    "\n",
    "X_encoded = vae.predict(train_data)\n",
    "\n",
    "# encode the train data\n",
    "X_train_encode_vae = vae.predict(X_train)\n",
    "\n",
    "# encode the test data\n",
    "X_test_encode_vae = vae.predict(X_test)\n",
    "\n",
    "# Train the decision tree model using the encoded features\n",
    "dtr_vae = DecisionTreeRegressor(max_depth = 30 , max_features = 'auto', min_samples_leaf = 30, min_samples_split = 20)\n",
    "dtr_vae.fit(X_train_encode_vae, y_train)\n",
    "\n",
    "def get_error_term(v1, v2, _rmse=True):\n",
    "    if _rmse:\n",
    "        return np.sqrt(np.mean((v1 - v2) ** 2))\n",
    "    #return MAE\n",
    "    return np.mean(abs(v1 - v2))\n",
    "\n",
    "X_train_pred = dtr_vae.predict(X_train_encode_vae)\n",
    "mae_vector_train = get_error_term(X_train_pred, y_train, _rmse=False)\n",
    "                   \n",
    "X_pred = dtr_vae.predict(X_test_encode_vae)\n",
    "mae_vector_test = get_error_term(X_pred, y_test, _rmse=False)\n",
    "                   \n",
    "\n",
    "#metrics \n",
    "print('MAE:', mean_absolute_error(y_test,X_pred))\n",
    "print('MSE:', mean_squared_error(y_test, X_pred))\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test, X_pred)))\n",
    "print('Train score:',dtr_vae.score(X_train_encode_vae,y_train))\n",
    "print('Test score:',dtr_vae.score(X_test_encode_vae,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6518f65-7310-4fa7-90ee-bc7075db9d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
